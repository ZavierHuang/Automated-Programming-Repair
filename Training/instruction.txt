【build ior】

C:/Users/islab/anaconda3/envs/tensorflow_env/python.exe f:/My_APR/Experiment_CodeLlama/repairllama/build_ior/build_dataset.py ^
-md F:/My_APR/Experiment_CodeLlama/repairllama/data/meta/ ^
-o F:/My_APR/Experiment_CodeLlama/repairllama/data/IR1OR1/ ^
-ir pbf ^
-or ff

====================================================================================================================================
  -h, --help            show this help message and exit
  --meta_data_path META_DATA_PATH, -md META_DATA_PATH
                        Path to the meta data.
  --output_path OUTPUT_PATH, -o OUTPUT_PATH
                        Path to the output.
  --input_representation {pbf,pbfwfl,pbfwc,pbfwln,pbfwcp,pbfwcpblc,pbfwcpblcc}, -ir {pbf,pbfwfl,pbfwc,pbfwln,pbfwcp,pbfwcpblc,pbfwcpblcc}
                        Pure buggy function: pbf/IR1, 
						Pure buggy function with fault location information: pbfwfl/IR2, 
						Pure buggy function with comments: pbfwc, Pure buggy function with line number: pbfwln, 
						Pure buggy function with cloze prompt: pbfwcp/IR3, 
						Pure buggy function with cloze prompt and buggy lines comments: pbfwcpblc/IR4, 
						Pure buggy function with cloze prompt and buggy lines comments and comments: pbfwcpblcc,
  --output_representation {fl,ldw/oc,ldwolc,ldwtlc,ff}, -or {fl,ldw/oc,ldwolc,ldwtlc,ff}
                        Fixed lines: fl/OR2, 
						Line diff without context: ldw/oc, 
						Line diff with one line context: ldwolc/OR4, 
						Line diff with three line context: 
						ldwtlc/OR3, 
						Full function: ff/OR1
====================================================================================================================================						
	

【fine-tuning】
W&B APR KEY = 90d2f45909946b5195b7ea94eb94a844079fac99
						
C:/Users/islab/anaconda3/envs/tensorflow_env/python.exe f:/My_APR/Experiment_CodeLlama/repairllama/src/lora/llama_sft.py ^
--is_lora True --model_max_length 1024 ^
--cache_path "F:/My_APR/Experiment_CodeLlama/repairllama/cache/" ^
--do_train ^
--do_eval False ^
--fp16 True ^
--num_train_epochs 2 ^
--per_device_train_batch_size 16 ^
--per_device_eval_batch_size 1 ^
--gradient_accumulation_steps 1 ^
--evaluation_strategy "no" ^
--eval_steps 10 ^
--save_steps 150 ^
--learning_rate 5e-4 ^
--lr_scheduler_type "cosine" ^
--logging_steps 10 ^
--ddp_find_unused_parameters False
--output_dir "F:/My_APR/Experiment_CodeLlama/repairllama/model/" ^
--data_path "json" --train_file "F:/My_APR/Experiment_CodeLlama/repairllama/data/IR3OR2/train_data.jsonl" ^
--eval_file "F:/My_APR/Experiment_CodeLlama/repairllama/data/IR3OR2/test_data.jsonl" ^
--model_name_or_path "C:/Users/islab/.cache/huggingface/hub/models--TheBloke--CodeLlama-7B-fp16/snapshots/ce09049eb9140a19cf78051cb5d849607b6fa8ec/" ^


NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported --> pip install -U datasets

ValueError：you cant pass 'load_in_4bit' or 'load_in_8bit' as a kwarg when passing 'quantization_config' augument at the same time --> make the version of transformers under 4.38.0 (ex:4.37.0)

fd = os.open(output_dir, os.O_RDONLY) --> fd = os.open(output_dir)


解决導入torch報錯from torch._C import xxxx
https://blog.csdn.net/weixin_35757704/article/details/124387389


pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
pip install transformers==4.37.0
pip install peft
pip install evaluate
pip install accelerate==0.28.0
pip install bitsandbytes


===========================TEST============================================
C:/Users/islab/anaconda3/envs/tensorflow_env/python.exe f:/My_APR/Experiment_CodeLlama/repairllama/src/lora/llama_sft.py ^
--data_path "json" ^
--train_file "F:/My_APR/Experiment_CodeLlama/repairllama/data/train_data.jsonl" ^
--eval_file "F:/My_APR/Experiment_CodeLlama/repairllama/data/test_data.jsonl" ^
--model_max_length 1024 ^
--cache_path "D:/CodeLlama/cache/" ^
--do_train ^
--do_eval False ^
--fp16 True ^
--output_dir "F:/My_APR/Experiment_CodeLlama/repairllama/model/" ^
--num_train_epochs 2 ^
--per_device_train_batch_size 1 ^
--per_device_eval_batch_size 1 ^
--gradient_accumulation_steps 1 ^
--evaluation_strategy "no" ^
--eval_steps 10 ^
--save_steps 20 ^
--learning_rate 5e-4 ^
--lr_scheduler_type "cosine" ^
--logging_steps 10 ^
--ddp_find_unused_parameters False ^
--is_lora True ^
--model_name_or_path "bigcode/starcoder2-7b"


=====================================================================================
Pred.py
C:/Users/islab/anaconda3/envs/tensorflow_env/python.exe f:/My_APR/Experiment_CodeLlama/repairllama/src/lora/llama_pred_utf-8.py ^
--do_sample True ^
--only_do_beam True ^
--only_do_topp False ^
--only_do_topk False ^
--only_do_temp False ^
--temperature 0.8 ^
--top_k 0 ^
--top_p 0.95 ^
--num_beams 1 ^
--request_num 1 ^
--data_path F:/My_APR/Experiment_CodeLlama/repairllama/Verification_HumanEval ^
--test_file HumanEvalPrograms_Qwen_IR4OR2_TEST.jsonl ^
--output_file F:/My_APR/Experiment_CodeLlama/repairllama/Verification_HumanEval_Output/Result_Qwen_Test/HumanEval_Qwen_Lora04_E1_Patch01.jsonl ^
--is_lora True ^
--lora_path F:/My_APR/Experiment_CodeLlama/repairllama/model_Qwen/model_Lora04/checkpoint-epoch-1.0/ ^
--base_model_path Qwen/Qwen2.5-Coder-1.5B

=====================================================CLOUD GPU 1===========================================================================
rank 4   -->  batchSize 8
rank 8   -->  batchSize 8
rank 16  -->  batchsize 16
===========================================================================================================================================
python ./src/llama_sft.py \
--data_path "json" \
--train_file "./data/IR4OR2/train_data.jsonl" \
--eval_file "./data/IR4OR2/test_data.jsonl" \
--cache_path "./cache/" \
--do_train \
--do_eval False \
--fp16 True \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 1 \
--evaluation_strategy "no" \
--eval_steps 10 \
--save_steps 150 \
--learning_rate 5e-4 \
--lr_scheduler_type "cosine" \
--logging_steps 10 \
--ddp_find_unused_parameters False \
--is_lora True \
--model_max_length 1024 \
--num_train_epochs 5 \
--per_device_train_batch_size 8 \
--output_dir "./model_Lora04/"


=====================================================CLOUD GPU 2===========================================================================
===========================================================================================================================================

python ./src/llama_sft2.py \
--data_path "json" \
--train_file "./data/IR4OR2_Copy/train_data.jsonl" \
--eval_file "./data/IR4OR2_Copy/test_data.jsonl" \
--cache_path "./cache/" \
--do_train \
--do_eval False \
--fp16 True \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 1 \
--evaluation_strategy "no" \
--eval_steps 10 \
--save_steps 150 \
--learning_rate 5e-4 \
--lr_scheduler_type "cosine" \
--logging_steps 10 \
--ddp_find_unused_parameters False \
--is_lora True \
--model_max_length 1024 \
--num_train_epochs 5 \
--per_device_train_batch_size 8 \
--output_dir "./model_Lora04/"





==================MODEL TRAIN TEST in CLOUD==============================================
python ./src/llama_sft_DeepAI.py \
--data_path "json" \
--train_file "./data/train_data.jsonl" \
--eval_file "./data/test_data.jsonl" \
--model_max_length 1024 \
--cache_path "./cacheDeepSeek/" \
--do_train \
--do_eval False \
--fp16 True \
--output_dir "./model_TEST/" \
--num_train_epochs 2 \
--per_device_train_batch_size 1 \
--per_device_eval_batch_size 1 \
--gradient_accumulation_steps 1 \
--evaluation_strategy "no" \
--eval_steps 10 \
--save_steps 20 \
--learning_rate 5e-4 \
--lr_scheduler_type "cosine" \
--logging_steps 10 \
--ddp_find_unused_parameters False \
--is_lora True



