{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFS + Flatten + LCS_LENGTH Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "============                                           ==============\n",
      "============                                           ==============\n",
      "============                                           ==============\n",
      "============      Model Init LORA:16 EPOCH:1          ==============\n",
      "============                                           ==============\n",
      "============                                           ==============\n",
      "============                                           ==============\n",
      "=====================================================================\n",
      "LORA:16 PATCH:01 EPOCH:1\n",
      "file_path: F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output/Result_Qwen_2.5/QuixBugs_Lora16/QuixBugs_Lora16_Patch01/QuixBugs_Qwen_Lora16_E1_Patch01.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patches:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORA:16 PATCH:01 EPOCH:1\n",
      "BREADTH_FIRST_SEARCH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\islab\\anaconda3\\envs\\tensorflow_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\islab\\anaconda3\\envs\\tensorflow_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 results: [' return false;\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patches: 100%|██████████| 40/40 [00:03<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output\\Analysis_BFS\\Module_BREADTH_FIRST_SEARCH To F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output\\Analysis_Pending\\Qwen2.5\\Lora16-E1-01\\Module_BREADTH_FIRST_SEARCH Successfully\n",
      "============================ Step1 LLM:Qwen/Qwen2.5-Coder-1.5B LORA:16 PATCH:01 EPOCH:1 Done =================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    GenerationConfig, \n",
    "    HfArgumentParser, \n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "def ModelInit(LLM, LORA, EPOCH):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=True,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model_folder_path = r'F:\\My_APR\\Experiment_CodeLlama\\repairllama\\model_Qwen\\model_Lora{}\\checkpoint-epoch-{}.0'.format(LORA, EPOCH)\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        model_folder_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.pad_token = tokenizer.pad_token = tokenizer.unk_token = tokenizer.eos_token\n",
    "    model.to(device)\n",
    "    print('=====================================================================')\n",
    "    print('============                                           ==============')\n",
    "    print('============                                           ==============')\n",
    "    print('============                                           ==============')\n",
    "    print('============      Model Init LORA:{} EPOCH:{}          =============='.format(LORA, EPOCH))\n",
    "    print('============                                           ==============')\n",
    "    print('============                                           ==============')\n",
    "    print('============                                           ==============')\n",
    "    print('=====================================================================')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def flatten_BFS_Beam_Search(model, tokenizer, buggy_Code, BEAM_NUM):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inputs = tokenizer(buggy_Code, return_tensors=\"pt\")\n",
    "    inputs_len = inputs[\"input_ids\"].shape[1]\n",
    "    inputs_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        num_beams=BEAM_NUM,\n",
    "        max_length = 512,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs_ids,\n",
    "        max_new_tokens=256,\n",
    "        num_return_sequences=BEAM_NUM, \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    output_ids = outputs[:, inputs_len:]\n",
    "    output_patch = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return output_patch\n",
    "\n",
    "def readJsonLine(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def createFolder(folder_path):\n",
    "    try:\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        os.mkdir(folder_path)\n",
    "    except:\n",
    "        print('remove {} error'.format(folder_path))\n",
    "\n",
    "def checkJavaFormat(java_code, jar_path, folder_path, patchFileName, buggy_ID):\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    os.chdir(script_dir)\n",
    "\n",
    "    if not os.path.isfile(jar_path):\n",
    "        return (f\"Google Java Format JAR file not found: {jar_path}\")\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".java\") as temp_file:\n",
    "        temp_filename = temp_file.name\n",
    "\n",
    "        full_java_code = f\"\"\"\n",
    "        public class {patchFileName} {{\n",
    "            {java_code}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        temp_file.write(full_java_code.encode())\n",
    "\n",
    "    if not os.path.isfile(temp_filename):\n",
    "        raise FileNotFoundError(f\"Temporary file not found: {temp_filename}\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\"java\", \"-jar\", jar_path, \"--replace\", temp_filename],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        return (f\"Google Java Format Error: {result.stderr}\")\n",
    "\n",
    "    with open(temp_filename, \"r\") as f:\n",
    "        formatted_code = f.read()\n",
    "\n",
    "    os.remove(temp_filename)\n",
    "\n",
    "    formatted_code = importContent(buggy_ID) + '\\n' + formatted_code\n",
    "\n",
    "    # print(\"PATH:\", folder_path + '/' + patchFileName + '.java')\n",
    "\n",
    "\n",
    "    with open(folder_path + '/' + patchFileName + '.java', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_code)\n",
    "\n",
    "    # print(formatted_code)\n",
    "\n",
    "    if 'Node' in formatted_code and 'WeightedEdge' in formatted_code:\n",
    "        return 'Node WeightedEdge'\n",
    "    if 'Node' in formatted_code:\n",
    "        return 'Node'\n",
    "    if 'WeightedEdge' in formatted_code:\n",
    "        return 'WeightedEdge'\n",
    "    \n",
    "    return 'Java Format Check Successfully'\n",
    "\n",
    "def checkJavaCompile(patchFilePath, javaFormatResult):\n",
    "    try:\n",
    "        output_dir = './class_file/'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        java_files = [patchFilePath]\n",
    "\n",
    "        if 'Node' in javaFormatResult:\n",
    "            java_files.append('F:/My_APR/QuixBugTest/dataStructure/Node.java')\n",
    "        \n",
    "        if 'WeightedEdge' in javaFormatResult:\n",
    "            java_files.append('F:/My_APR/QuixBugTest/dataStructure/WeightedEdge.java')\n",
    "\n",
    "        result = subprocess.run(['javac', '-d', output_dir] + java_files, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: javac is not installed or not found in PATH.\")\n",
    "        return False\n",
    "\n",
    "def importContent(fileName):\n",
    "    import_folder_path = \"F:/My_APR/QuixBugs_Program/eachFileImport/\"\n",
    "    import_content = \"\"\n",
    "    file_import_path = import_folder_path + fileName + '_ImportInfo.java'\n",
    "\n",
    "    with open(file_import_path, 'r', encoding='utf-8') as importFile:\n",
    "        for line in importFile:\n",
    "            import_content += line\n",
    "\n",
    "    return import_content\n",
    "\n",
    "def list_subdirectories(directory):\n",
    "        subdirectories = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for dir_name in dirs:\n",
    "                subdirectories.append(os.path.join(root, dir_name))\n",
    "        return subdirectories\n",
    "\n",
    "def ClearAllModuleFolder():\n",
    "    # 範例用法\n",
    "    folder_path = \"F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output/Analysis/\"  # 替換為你的資料夾路徑\n",
    "    all_subdirectories = list_subdirectories(folder_path)\n",
    "\n",
    "    module_subdirectories = [subfolder for subfolder in all_subdirectories if subfolder[subfolder.rfind('/')+1:].startswith('Module') is True]\n",
    "    for subdirectory in module_subdirectories:\n",
    "        createFolder(subdirectory)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    LLM = 'Qwen/Qwen2.5-Coder-1.5B'\n",
    "    LORA = '16'\n",
    "\n",
    "    EPOCH_LIST = ['1']\n",
    "    PATCH_LIST = ['01']\n",
    "\n",
    "    # EPOCH_LIST = ['1', '2', '3', '4', '5']\n",
    "    # PATCH_LIST = ['01', '05', '10']\n",
    "    \n",
    "    MIDDLE = '<|fim_middle|>'\n",
    "    SUFFIX = '<|fim_suffix|>'\n",
    "    PREFIX = '<|fim_prefix|>'\n",
    "\n",
    "    for EPOCH in EPOCH_LIST:\n",
    "        \n",
    "        model, tokenizer = ModelInit(LLM, LORA, EPOCH)\n",
    "\n",
    "        for PATCH in PATCH_LIST:\n",
    "\n",
    "            print(\"LORA:{} PATCH:{} EPOCH:{}\".format(LORA, PATCH, EPOCH))\n",
    "            \n",
    "            ClearAllModuleFolder()\n",
    "\n",
    "            MAIN_PATH = 'F:/My_APR/Experiment_CodeLlama/repairllama/'\n",
    "            json_file_path = r'Verification_QuixBugs_Output\\Result_Qwen_2.5\\QuixBugs_Lora{}\\QuixBugs_Lora{}_Patch{}\\QuixBugs_Qwen_Lora{}_E{}_Patch{}.jsonl'.format(LORA, LORA, PATCH, LORA, EPOCH, PATCH)\n",
    "\n",
    "            file_path = MAIN_PATH + json_file_path.replace('\\\\', '/')\n",
    "\n",
    "            print(\"file_path:\", file_path)\n",
    "\n",
    "\n",
    "            google_java_format_path = \"F:/My_APR/util/javaFormat/google-java-format-1.15.0-all-deps.jar\"\n",
    "\n",
    "            data = readJsonLine(file_path)\n",
    "\n",
    "            pendingList = ['BREADTH_FIRST_SEARCH']\n",
    "            # pendingList = ['BREADTH_FIRST_SEARCH', 'FLATTEN', 'LCS_LENGTH']\n",
    "\n",
    "            \n",
    "            with tqdm(total=len(data), desc=\"Processing Patches\") as pbar:\n",
    "\n",
    "                for item in data:\n",
    "                    index = 0\n",
    "\n",
    "                    buggy_ID = item['bug_id']\n",
    "                    buggy_Code = item['buggy_code']\n",
    "                    folder_path = 'F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output/Analysis/Module_{}'.format(buggy_ID)\n",
    "\n",
    "                    if buggy_ID not in pendingList:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    print(\"LORA:{} PATCH:{} EPOCH:{}\".format(LORA, PATCH, EPOCH))\n",
    "                    createFolder(folder_path)        \n",
    "                        \n",
    "                    print(buggy_ID)\n",
    "\n",
    "                    BEAM_NUM = len(item['output'])\n",
    "\n",
    "                    for i in range(BEAM_NUM):\n",
    "                        patch = item['output'][str(i)]['output_patch']\n",
    "                        patch = patch.replace('</s>', '')\n",
    "                        patch = patch.strip()\n",
    "\n",
    "                        # Qwen2.5\n",
    "                        buggyCode = buggy_Code.replace(SUFFIX, patch, 1)\n",
    "                        buggyCode = buggyCode[:buggyCode.find(MIDDLE)+len(MIDDLE)]\n",
    "                        TwiceBuggy = buggyCode[buggyCode.find('// buggy code') : buggyCode.find('// fill') + len('// fill')]\n",
    "                        buggyCode = buggyCode.replace(TwiceBuggy, SUFFIX)\n",
    "                        patchCode = buggyCode + '\\n' + TwiceBuggy[:-len('// fill')]\n",
    "\n",
    "                        # print(\"Patch Code:\", patchCode)\n",
    "\n",
    "                        results = flatten_BFS_Beam_Search(model, tokenizer, patchCode, BEAM_NUM)\n",
    "\n",
    "                        print(\"i:\",i,\"results:\",results)\n",
    "\n",
    "                        for result in results:\n",
    "                            patchFileName = buggy_ID + '_TEST_' + str(index)\n",
    "                            patchFilePath = folder_path + '/' + patchFileName + '.java'\n",
    "\n",
    "\n",
    "                            patchCodeTwice = patchCode.replace(SUFFIX, result.replace('</s>','').strip(), 1)\n",
    "                            patchCodeTwice = patchCodeTwice.replace(PREFIX, '')\n",
    "                            patchCodeTwice = patchCodeTwice.replace(patchCode[patchCode.find(MIDDLE):], '')\n",
    "                            javaFormatResult = checkJavaFormat(patchCodeTwice, google_java_format_path, folder_path, patchFileName, buggy_ID)\n",
    "\n",
    "                            if javaFormatResult.startswith('Google Java Format Error'):\n",
    "                                # print(javaFormatResult)\n",
    "                                continue\n",
    "                            \n",
    "                            checkCompileResult = checkJavaCompile(folder_path + '/' + patchFileName + '.java', javaFormatResult)\n",
    "                            # print(checkCompileResult)\n",
    "                            # print(index,folder_path + '/' + patchFileName + '.java')\n",
    "                            \n",
    "                            if checkCompileResult is False:\n",
    "                                os.remove(patchFilePath)\n",
    "\n",
    "                            index = index + 1\n",
    "                        \n",
    "                    pbar.update(1)\n",
    "\n",
    "            for Module_Folder in pendingList:   \n",
    "                source_folder = MAIN_PATH + r\"Verification_QuixBugs_Output\\Analysis_BFS\\Module_{}\".format(Module_Folder)\n",
    "                destination_base_folder = MAIN_PATH + r\"Verification_QuixBugs_Output\\Analysis_Pending\\Qwen2.5\\Lora{}-E{}-{}\".format(LORA, EPOCH, PATCH)\n",
    "\n",
    "                # 確保目標資料夾存在\n",
    "                os.makedirs(destination_base_folder, exist_ok=True)\n",
    "\n",
    "                # 獲取來源資料夾的名稱\n",
    "                folder_name = os.path.basename('Module_' + Module_Folder.rstrip(\"/\\\\\"))\n",
    "                # 定義目標路徑\n",
    "                destination_folder = os.path.join(destination_base_folder, folder_name)\n",
    "                try:\n",
    "                    shutil.copytree(source_folder, destination_folder)\n",
    "                    print(f\"Copy {source_folder} To {destination_folder} Successfully\")\n",
    "                except FileExistsError:\n",
    "                    print(f\"{destination_folder} Existed\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Copy Failure\")\n",
    "            \n",
    "            print(\"============================ Step1 LLM:{} LORA:{} PATCH:{} EPOCH:{} Done =================================\\n\\n\".format(LLM, LORA, PATCH, EPOCH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy 3 Module_Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Module_Folder in pendingList:   \n",
    "        source_folder = MAIN_PATH + r\"Verification_QuixBugs_Output\\Analysis_BFS\\Module_{}\".format(Module_Folder)\n",
    "        destination_base_folder = MAIN_PATH + r\"Verification_QuixBugs_Output\\Analysis_Pending\\DeepSeekCoder1.3B\\Lora{}-E{}-{}\".format(LORA, EPOCH, PATCH)\n",
    "\n",
    "        # 確保目標資料夾存在\n",
    "        os.makedirs(destination_base_folder, exist_ok=True)\n",
    "\n",
    "        # 獲取來源資料夾的名稱\n",
    "        folder_name = os.path.basename(Module_Folder.rstrip(\"/\\\\\"))\n",
    "        # 定義目標路徑\n",
    "        destination_folder = os.path.join(destination_base_folder, folder_name)\n",
    "        try:\n",
    "            shutil.copytree(source_folder, destination_folder)\n",
    "            print(f\"Copy {source_folder} To {destination_folder} Successfully\")\n",
    "        except FileExistsError:\n",
    "            print(f\"{destination_folder} Existed\")\n",
    "        except Exception as e:\n",
    "             print(f\"Copy Failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepSeekCoder Twice Fill Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<｜fim▁begin｜>public static Object flatten(Object arr) {\n",
      "    if (arr instanceof ArrayList) {\n",
      "        ArrayList narr = (ArrayList) arr;\n",
      "        ArrayList result = new ArrayList(50);\n",
      "        for (Object x : narr) {\n",
      "            if (x instanceof ArrayList) {\n",
      "                result.addAll((ArrayList) flatten(x));\n",
      "            } else {\n",
      "                result.add(x);\n",
      "            }\n",
      "        }\n",
      "        return result;\n",
      "    } else {\n",
      "        <｜fim▁hole｜>\n",
      "    }\n",
      "}\n",
      "\n",
      "<｜fim▁end｜>\n",
      "// buggy code\n",
      "        // return flatten(arr);\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "buggyCode = \"\"\"\n",
    "<｜fim▁begin｜>public static Object flatten(Object arr) {\n",
    "    if (arr instanceof ArrayList) {\n",
    "        ArrayList narr = (ArrayList) arr;\n",
    "        ArrayList result = new ArrayList(50);\n",
    "        for (Object x : narr) {\n",
    "            if (x instanceof ArrayList) {\n",
    "                result.addAll((ArrayList) flatten(x));\n",
    "            } else {\n",
    "                result.add(x);\n",
    "            }\n",
    "        }\n",
    "        return result;\n",
    "    } else {\n",
    "        // buggy code\n",
    "        // return flatten(arr);\n",
    "        // fill\n",
    "    }\n",
    "}\n",
    "\n",
    "<｜fim▁end｜>\n",
    "// buggy code\n",
    "                // result.add(flatten(x));\n",
    "\"\"\"\n",
    "\n",
    "buggyCode = buggyCode[:buggyCode.find('<｜fim▁end｜>')+len('<｜fim▁end｜>')]\n",
    "TwiceBuggy = buggyCode[buggyCode.find('// buggy code') : buggyCode.find('// fill') + len('// fill')]\n",
    "buggyCode = buggyCode.replace(TwiceBuggy, '<｜fim▁hole｜>')\n",
    "buggyCode = buggyCode + '\\n' + TwiceBuggy[:-len('// fill')]\n",
    "print(buggyCode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
