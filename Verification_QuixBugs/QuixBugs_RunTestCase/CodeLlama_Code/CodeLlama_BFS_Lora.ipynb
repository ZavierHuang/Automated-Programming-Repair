{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFS + FLATTEN + LCS_LENGTH Config ---> 4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\islab\\anaconda3\\envs\\tensorflow_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9900f11dc0f24e89bb3a1f6a7d6f4082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32016, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    GenerationConfig, \n",
    "    HfArgumentParser, \n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "# If you need to use a specific GPU, you can set it here\n",
    "# if torch.cuda.is_available():\n",
    "#     # Set GPU:1 as the device\n",
    "#     torch.cuda.set_device(1)\n",
    "#     print(f\"Using GPU: {torch.cuda.current_device()}\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available.\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-hf\",\n",
    "    torch_dtype=torch.float32,\n",
    "    # load_in_8bit=True,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "LORA = '16'\n",
    "EPOCH = 1\n",
    "model_folder_path = 'F:/My_APR/Experiment_CodeLlama/repairllama/model_CodeLlama/'\n",
    "lora_folder_path = 'model_Lora{}/'.format(LORA)\n",
    "epoch_nums = 'checkpoint-epoch-{}.0/'.format(EPOCH)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_folder_path + lora_folder_path + epoch_nums,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "model.config.pad_token = tokenizer.pad_token = tokenizer.unk_token\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_BFS_Beam_Search(buggy_Code, BEAM_NUM):\n",
    "\n",
    "    inputs = tokenizer(buggy_Code, return_tensors=\"pt\")\n",
    "    inputs_len = inputs[\"input_ids\"].shape[1]\n",
    "    inputs_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        num_beams=BEAM_NUM,\n",
    "        max_length = 512,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs_ids,\n",
    "        max_new_tokens=256,\n",
    "        num_return_sequences=BEAM_NUM, \n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    output_ids = outputs[:, inputs_len:]\n",
    "    output_patch = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    return output_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFS + Flatten + LCS_LENGTH Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../QuixBugs_Lora16/QuixBugs_Lora16_Patch01/QuixBugs_Lora16_E1_Patch01.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../QuixBugs_Lora\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/QuixBugs_Lora\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_Patch\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/QuixBugs_Lora\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_E\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_Patch\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(LORA, LORA, PATCH, LORA, EPOCH, PATCH)\n\u001b[0;32m    118\u001b[0m google_java_format_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/My_APR/util/javaFormat/google-java-format-1.15.0-all-deps.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mreadJsonLine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m pendingList \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBREADTH_FIRST_SEARCH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLATTEN\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;66;03m# 'LCS_LENGTH']\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# pendingList = ['LCS_LENGTH']\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mreadJsonLine\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadJsonLine\u001b[39m(file_path):\n\u001b[0;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     12\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n",
      "File \u001b[1;32mc:\\Users\\islab\\anaconda3\\envs\\tensorflow_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../QuixBugs_Lora16/QuixBugs_Lora16_Patch01/QuixBugs_Lora16_E1_Patch01.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def readJsonLine(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def createFolder(folder_path):\n",
    "    try:\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        os.mkdir(folder_path)\n",
    "    except:\n",
    "        print('remove {} error'.format(folder_path))\n",
    "\n",
    "def checkJavaFormat(java_code, jar_path, folder_path, patchFileName, buggy_ID):\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "    os.chdir(script_dir)\n",
    "\n",
    "    if not os.path.isfile(jar_path):\n",
    "        return (f\"Google Java Format JAR file not found: {jar_path}\")\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".java\") as temp_file:\n",
    "        temp_filename = temp_file.name\n",
    "\n",
    "        full_java_code = f\"\"\"\n",
    "        public class {patchFileName} {{\n",
    "            {java_code}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        temp_file.write(full_java_code.encode())\n",
    "\n",
    "    if not os.path.isfile(temp_filename):\n",
    "        raise FileNotFoundError(f\"Temporary file not found: {temp_filename}\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [\"java\", \"-jar\", jar_path, \"--replace\", temp_filename],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        return (f\"Google Java Format Error: {result.stderr}\")\n",
    "\n",
    "    with open(temp_filename, \"r\") as f:\n",
    "        formatted_code = f.read()\n",
    "\n",
    "    os.remove(temp_filename)\n",
    "\n",
    "    formatted_code = importContent(buggy_ID) + '\\n' + formatted_code\n",
    "\n",
    "    print(\"PATH:\", folder_path + '/' + patchFileName + '.java')\n",
    "\n",
    "\n",
    "    with open(folder_path + '/' + patchFileName + '.java', 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_code)\n",
    "\n",
    "    # print(formatted_code)\n",
    "\n",
    "    if 'Node' in formatted_code and 'WeightedEdge' in formatted_code:\n",
    "        return 'Node WeightedEdge'\n",
    "    if 'Node' in formatted_code:\n",
    "        return 'Node'\n",
    "    if 'WeightedEdge' in formatted_code:\n",
    "        return 'WeightedEdge'\n",
    "    \n",
    "    return 'Java Format Check Successfully'\n",
    "\n",
    "def checkJavaCompile(patchFilePath, javaFormatResult):\n",
    "    try:\n",
    "        output_dir = './class_file/'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        java_files = [patchFilePath]\n",
    "\n",
    "        if 'Node' in javaFormatResult:\n",
    "            java_files.append('F:/My_APR/QuixBugTest/dataStructure/Node.java')\n",
    "        \n",
    "        if 'WeightedEdge' in javaFormatResult:\n",
    "            java_files.append('F:/My_APR/QuixBugTest/dataStructure/WeightedEdge.java')\n",
    "\n",
    "        result = subprocess.run(['javac', '-d', output_dir] + java_files, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: javac is not installed or not found in PATH.\")\n",
    "        return False\n",
    "\n",
    "def importContent(fileName):\n",
    "    import_folder_path = \"F:/My_APR/QuixBugs_Program/eachFileImport/\"\n",
    "    import_content = \"\"\n",
    "    file_import_path = import_folder_path + fileName + '_ImportInfo.java'\n",
    "\n",
    "    with open(file_import_path, 'r', encoding='utf-8') as importFile:\n",
    "        for line in importFile:\n",
    "            import_content += line\n",
    "\n",
    "    return import_content\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    LORA = '16'\n",
    "    PATCH = '01'\n",
    "    EPOCH = '1'\n",
    "    \n",
    "    file_path = '../QuixBugs_Lora{}/QuixBugs_Lora{}_Patch{}/QuixBugs_Lora{}_E{}_Patch{}.jsonl'.format(LORA, LORA, PATCH, LORA, EPOCH, PATCH)\n",
    "    google_java_format_path = \"F:/My_APR/util/javaFormat/google-java-format-1.15.0-all-deps.jar\"\n",
    "\n",
    "    data = readJsonLine(file_path)\n",
    "\n",
    "    pendingList = ['BREADTH_FIRST_SEARCH', 'FLATTEN']# 'LCS_LENGTH']\n",
    "    # pendingList = ['LCS_LENGTH']\n",
    "\n",
    "    \n",
    "    with tqdm(total=len(data), desc=\"Processing Patches\") as pbar:\n",
    "\n",
    "        for item in data:\n",
    "            index = 0\n",
    "\n",
    "            buggy_ID = item['bug_id']\n",
    "            buggy_Code = item['buggy_code']\n",
    "            folder_path = 'F:/My_APR/Experiment_CodeLlama/repairllama/Verification_QuixBugs_Output/Analysis/Module_{}'.format(buggy_ID)\n",
    "\n",
    "            if buggy_ID not in pendingList:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            print(\"folder_path:\", folder_path)\n",
    "            print(\"LORA:{} PATCH:{} EPOCH:{}\".format(LORA, PATCH, EPOCH))\n",
    "            createFolder(folder_path)        \n",
    "                \n",
    "            print(buggy_ID)\n",
    "\n",
    "            BEAM_NUM = len(item['output'])\n",
    "\n",
    "            for i in range(BEAM_NUM):\n",
    "                patch = item['output'][str(i)]['output_patch']\n",
    "                patch = patch.replace('</s>', '')\n",
    "                patch = patch.strip()\n",
    "                patchCode = buggy_Code.replace('<FILL_ME>', patch, 1)\n",
    "                patchCode = patchCode.replace('// buggy code', '', 1)\n",
    "\n",
    "                # print(\"Patch Code:\", patchCode)\n",
    "\n",
    "                results = flatten_BFS_Beam_Search(patchCode, BEAM_NUM)\n",
    "\n",
    "                print(\"i:\",i,\"results:\",results)\n",
    "\n",
    "                for result in results:\n",
    "                    patchFileName = buggy_ID + '_TEST_' + str(index)\n",
    "                    patchFilePath = folder_path + '/' + patchFileName + '.java'\n",
    "\n",
    "\n",
    "                    patchCodeTwice = patchCode.replace('<FILL_ME>', result.replace('</s>','').strip(), 1)\n",
    "                    javaFormatResult = checkJavaFormat(patchCodeTwice, google_java_format_path, folder_path, patchFileName, buggy_ID)\n",
    "\n",
    "                    if javaFormatResult.startswith('Google Java Format Error'):\n",
    "                        # print(javaFormatResult)\n",
    "                        continue\n",
    "                    \n",
    "                    checkCompileResult = checkJavaCompile(folder_path + '/' + patchFileName + '.java', javaFormatResult)\n",
    "                    # print(checkCompileResult)\n",
    "                    # print(index,folder_path + '/' + patchFileName + '.java')\n",
    "                    \n",
    "                    if checkCompileResult is False:\n",
    "                        os.remove(patchFilePath)\n",
    "\n",
    "                    index = index + 1\n",
    "                \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"============================ Step1 LORA:{} PATCH:{} EPOCH:{} Done =================================\".format(LORA, PATCH, EPOCH))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
